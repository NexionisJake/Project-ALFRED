import os
import sys
import asyncio
import time
import re
import json
import edge_tts
import pygame
import keyboard  # For keyboard interrupt support
from colorama import Fore, init
from dotenv import load_dotenv
from langchain_groq import ChatGroq
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from collections import deque
from PyQt6.QtWidgets import QApplication
from PyQt6.QtCore import QThread, pyqtSignal

# IMPORTS FROM YOUR OTHER FILES
from core.tools import (open_application, get_system_status, google_search, system_volume,
                   media_play_pause, media_next, media_previous, search_knowledge_base,
                   get_current_time, get_weather, write_to_screen)
from core.ears import listen_and_transcribe, listen_for_wake_word, OPENWAKEWORD_AVAILABLE
from core.eyes import take_screenshot
from core.overlay import OverlayWindow, COLOR_HAPPY, COLOR_ALERT, COLOR_ERROR, COLOR_NEUTRAL
import config

init(autoreset=True)

# Initialize pygame mixer for audio playback
pygame.mixer.init()

# --- LOAD ENVIRONMENT VARIABLES ---
load_dotenv()

api_key = os.getenv("GROQ_API_KEY")
if not api_key:
    print(Fore.RED + "ERROR: GROQ_API_KEY not found in .env file")
    sys.exit(1)

os.environ["GROQ_API_KEY"] = api_key

# --- CONFIG ---
# Using limited memory to prevent overflow
chat_memory = deque(maxlen=config.MAX_MEMORY_DEPTH)

# Memory optimization tracking
message_count = 0
conversation_summary = None  # Stores condensed conversation context

# Voice from config
VOICE = config.VOICE_NAME

# --- LONG-TERM MEMORY PERSISTENCE ---
MEMORY_FILE = os.path.join(os.path.dirname(__file__), "data", "long_term_memory.json")

def save_memory():
    """Save chat memory to JSON file (serialization)"""
    try:
        memory_data = []
        for msg in chat_memory:
            # Serialize message objects to dict
            msg_dict = {
                "type": msg.__class__.__name__,
                "content": msg.content
            }
            memory_data.append(msg_dict)
        
        with open(MEMORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(memory_data, f, indent=2)
    except Exception as e:
        print(Fore.YELLOW + f"Warning: Could not save memory: {e}")

def load_memory():
    """Load chat memory from JSON file (deserialization)"""
    try:
        if not os.path.exists(MEMORY_FILE):
            print(Fore.CYAN + "No previous memory found. Starting fresh.")
            return
        
        with open(MEMORY_FILE, 'r', encoding='utf-8') as f:
            memory_data = json.load(f)
        
        # Deserialize dict back to message objects
        for msg_dict in memory_data:
            if msg_dict["type"] == "HumanMessage":
                chat_memory.append(HumanMessage(content=msg_dict["content"]))
            elif msg_dict["type"] == "AIMessage":
                chat_memory.append(AIMessage(content=msg_dict["content"]))
            elif msg_dict["type"] == "SystemMessage":
                chat_memory.append(SystemMessage(content=msg_dict["content"]))
        
        print(Fore.GREEN + f"‚úî Loaded {len(memory_data)} messages from previous session")
    except Exception as e:
        print(Fore.YELLOW + f"Warning: Could not load memory: {e}") 

def clean_text_for_speech(text):
    """Remove markdown and special characters that shouldn't be spoken."""
    # Remove asterisks (bold/italic markdown)
    text = text.replace('*', '')
    
    # Remove hashtags
    text = text.replace('#', '')
    
    # Remove underscores
    text = text.replace('_', '')
    
    # Remove brackets
    text = text.replace('[', '').replace(']', '')
    
    # Remove parentheses content that looks like citations or references
    text = re.sub(r'\([^)]*\)', '', text)
    
    # Remove multiple spaces
    text = re.sub(r'\s+', ' ', text)
    
    # Remove code blocks and backticks
    text = text.replace('`', '')
    
    return text.strip()

def speak(text):
    """Converts text to speech using Microsoft Edge TTS."""
    if text.startswith("["):
        return
    
    # Clean text before speaking
    clean_text = clean_text_for_speech(text)
    
    print(Fore.YELLOW + f"üîä Speaking: '{clean_text[:50]}...'")
    sys.stdout.flush()
    
    try:
        # Generate speech file with cleaned text
        asyncio.run(async_speak(clean_text))
        
        # Unload any previous audio to release file lock
        pygame.mixer.music.unload()
        
        # Play audio synchronously (blocks until complete)
        pygame.mixer.music.load("temp_speech.mp3")
        pygame.mixer.music.play()
        
        print(Fore.GREEN + "‚úì Playing audio...")
        sys.stdout.flush()
        
        # Wait until audio finishes playing (with keyboard interrupt support)
        print(Fore.CYAN + "üí° Press ESC to interrupt speech")
        while pygame.mixer.music.get_busy():
            # Check for ESC key to stop speech
            if keyboard.is_pressed('esc'):
                print(Fore.YELLOW + "‚è∏Ô∏è Speech interrupted by user")
                pygame.mixer.music.stop()
                break
            time.sleep(0.1)
        
        # Unload to release file lock for next time
        pygame.mixer.music.unload()
        
        print(Fore.CYAN + "‚úì Speech completed, ready to listen")
        sys.stdout.flush()
        
        # Small buffer to ensure microphone doesn't pick up tail end
        time.sleep(0.5)
        
    except Exception as e:
        print(Fore.RED + f"Speech Error: {e}")
        pygame.mixer.music.unload()  # Cleanup on error too
        sys.stdout.flush()

async def async_speak(text):
    """Async helper for edge-tts."""
    communicate = edge_tts.Communicate(text, VOICE)
    await communicate.save("temp_speech.mp3")

def summarize_conversation():
    """Summarizes the conversation history to reduce token usage."""
    global conversation_summary, message_count
    
    if len(chat_memory) < 5:  # Not enough to summarize
        return
    
    print(Fore.MAGENTA + "üîÑ Optimizing memory (summarizing conversation)...")
    
    try:
        # Build conversation text from memory
        conversation_text = "\n".join([
            f"{msg.__class__.__name__}: {msg.content}" 
            for msg in chat_memory
        ])
        
        # Ask cloud brain to summarize
        summary_prompt = SystemMessage(content="""Summarize this conversation in 2-3 sentences. 
        Focus on key facts, user preferences, and ongoing topics. Be concise.""")
        
        response = cloud_brain.invoke([
            summary_prompt,
            HumanMessage(content=f"Conversation to summarize:\n{conversation_text}")
        ])
        
        conversation_summary = response.content
        print(Fore.GREEN + f"‚úì Memory optimized. Summary: {conversation_summary[:60]}...")
        
        # Clear old messages but keep the summary as context
        chat_memory.clear()
        chat_memory.append(SystemMessage(content=f"Previous conversation summary: {conversation_summary}"))
        
        # Reset counter
        message_count = 0
        
    except Exception as e:
        print(Fore.YELLOW + f"‚ö† Summarization failed: {e}")

print(Fore.YELLOW + "Initializing Ajax System...")

try:
    cloud_brain = ChatGroq(model=config.CLOUD_MODEL, temperature=0.7)
    local_body = ChatOllama(model=config.LOCAL_MODEL, temperature=0)
    
    # Bind tools to Local Body (now includes 11 tools)
    tools_list = [open_application, get_system_status, google_search, system_volume,
                  media_play_pause, media_next, media_previous, search_knowledge_base,
                  get_current_time, get_weather, write_to_screen]
    local_body_with_tools = local_body.bind_tools(tools_list)
    print(Fore.CYAN + "‚úî Systems Online (Cloud + Local + 11 Tools)")
except Exception as e:
    print(Fore.RED + f"‚úò Init Error: {e}")
    sys.exit()

# Load previous conversation memory from disk
load_memory()

# --- THE ROUTER WITH VISION ---
def process_command(user_input, ui_callback=None):
    # Print what we received to confirm input works
    print(Fore.WHITE + f"DEBUG: Processing input -> '{user_input}'")
    
    # 1. Vision Check (Does the user want us to LOOK?)
    vision_keywords = config.VISION_KEYWORDS
    wants_vision = any(k in user_input.lower() for k in vision_keywords)

    if wants_vision:
        print(Fore.MAGENTA + "üëÄ Vision Mode Activated...")
        speak("Taking a look.")
        
        # Capture Image
        image_data = take_screenshot()
        
        # Prepare Message for Groq Vision Model
        msg = HumanMessage(
            content=[
                {"type": "text", "text": user_input},
                {"type": "image_url", "image_url": {"url": image_data}}
            ]
        )
        
        # Send to Cloud Brain (Vision Model)
        try:
            vision_model = ChatGroq(model=config.VISION_MODEL, temperature=0.5)
            response = vision_model.invoke([msg])
            
            content = response.content
            print(Fore.CYAN + f"[Vision]: {content}")
            
            # Vision gets neutral color (informational)
            if ui_callback:
                ui_callback(content, COLOR_NEUTRAL)
            
            # Save context to memory (as text description)
            chat_memory.append(HumanMessage(content=f"User showed an image. You saw: {content}"))
            
            # Check if user also wants to WRITE/PASTE the code
            write_keywords = ["write", "type", "paste", "input", "code it", "implement"]
            wants_to_write = any(k in user_input.lower() for k in write_keywords)
            
            print(Fore.WHITE + f"DEBUG: wants_to_write={wants_to_write}, user_input='{user_input.lower()}'")
            
            if wants_to_write:
                print(Fore.GREEN + "‚úç Extracting code from vision response to paste...")
                
                # Extract code block from vision response
                code_match = re.search(r'```(?:python|cpp|java|javascript)?\n(.*?)```', content, re.DOTALL)
                
                if code_match:
                    code_to_write = code_match.group(1).strip()
                    print(Fore.GREEN + f"‚ñ∂ Executing Tool: write_to_screen")
                    print(Fore.CYAN + f"Code to paste:\n{code_to_write}")
                    tool_result = write_to_screen.invoke({"text": code_to_write})
                    print(Fore.GREEN + f"‚úî Result: {tool_result}")
                    speak("Code pasted successfully, Sir.")
                else:
                    print(Fore.YELLOW + "‚ö† No code block found in vision response")
                    speak("I analyzed the problem, Sir, but couldn't extract a clean code block to paste.")
            else:
                # No write request, just speak the analysis
                speak(content)
            
            return content
            
        except Exception as e:
            print(Fore.RED + f"Vision Error: {e}")
            if ui_callback:
                ui_callback("I had trouble seeing that.", COLOR_ERROR)
            speak("I had trouble seeing that.")
            return "Error"
    
    # 2. Standard Logic - Add to memory for non-vision queries
    global message_count
    chat_memory.append(HumanMessage(content=user_input))
    message_count += 1
    
    # Check if we need to summarize (every 10 messages)
    if message_count >= 10:
        summarize_conversation()
    
    system_keywords = config.SYSTEM_KEYWORDS
    question_starters = config.QUESTION_STARTERS
    
    # Knowledge keywords that should ALWAYS route to tools (even if questions)
    knowledge_keywords = ["what is my", "tell me about my", "my wifi", "my password", 
                         "my name", "my favorite", "my dog", "my pet", "remember"]
    
    has_keyword = any(k in user_input.lower() for k in system_keywords)
    is_question = any(user_input.lower().strip().startswith(s) for s in question_starters)
    needs_knowledge = any(k in user_input.lower() for k in knowledge_keywords)

    print(Fore.WHITE + f"DEBUG: Keyword={has_keyword}, Question={is_question}, Knowledge={needs_knowledge}")

    # Route to LOCAL BODY if: has keyword OR needs knowledge base
    # (System keywords ALWAYS route to tools, even if phrased as questions)
    if has_keyword or needs_knowledge:
        print(Fore.YELLOW + "‚ö° Routing to LOCAL SYSTEM...")
        
        system_instruction = SystemMessage(content="""
        You are a PC Automation Agent with access to tools. You MUST use the appropriate tool for every request.
        
        CRITICAL RULES:
        1. NEVER respond with text explanations when a tool is available
        2. ALWAYS call the appropriate tool immediately
        3. Do NOT say "I don't have access" or "I suggest checking" - USE THE TOOL
        
        TOOL USAGE:
        - Open/launch/start apps ‚Üí use open_application tool
        - System status/CPU/RAM ‚Üí use get_system_status tool
        - Google search ‚Üí use google_search tool
        - Volume/sound ‚Üí use system_volume tool
        - Media controls (play/pause/skip) ‚Üí use media tools
        - Weather/temperature/forecast ‚Üí ALWAYS use get_weather tool (you DO have this capability)
        - Time/date ‚Üí use get_current_time tool
        - Write/type/code ‚Üí use write_to_screen tool
        - Personal info (my name, password, favorites) ‚Üí use search_knowledge_base tool
        
        EXAMPLE:
        User: "What's the weather in Bangalore?"
        You: Call get_weather with city="Bangalore" (DO NOT respond with text!)
        """)
        
        recent_memory = list(chat_memory)[-5:]
        ai_msg = local_body_with_tools.invoke([system_instruction] + recent_memory)
        
        print(Fore.CYAN + f"DEBUG: Local Body Response - Tool Calls: {ai_msg.tool_calls}")
        print(Fore.CYAN + f"DEBUG: Local Body Content: {ai_msg.content}")
        
        # Tool Execution Logic
        if ai_msg.tool_calls:
            for tool_call in ai_msg.tool_calls:
                tool_name = tool_call["name"]
                tool_args = tool_call["args"]
                print(Fore.GREEN + f"‚ñ∂ Executing Tool: {tool_name}")
                
                # Execute Tool
                if tool_name == "open_application":
                    tool_result = open_application.invoke(tool_args)
                elif tool_name == "get_system_status":
                    tool_result = get_system_status.invoke(tool_args)
                elif tool_name == "google_search":
                    tool_result = google_search.invoke(tool_args)
                elif tool_name == "system_volume":
                    tool_result = system_volume.invoke(tool_args)
                elif tool_name == "media_play_pause":
                    tool_result = media_play_pause.invoke(tool_args)
                elif tool_name == "media_next":
                    tool_result = media_next.invoke(tool_args)
                elif tool_name == "media_previous":
                    tool_result = media_previous.invoke(tool_args)
                elif tool_name == "search_knowledge_base":
                    tool_result = search_knowledge_base.invoke(tool_args)
                elif tool_name == "get_current_time":
                    tool_result = get_current_time.invoke(tool_args)
                elif tool_name == "get_weather":
                    tool_result = get_weather.invoke(tool_args)
                elif tool_name == "write_to_screen":
                    tool_result = write_to_screen.invoke(tool_args)
                else:
                    tool_result = "Unknown Tool"
                
                print(Fore.GREEN + f"‚úî Result: {tool_result}")
                
                # Determine sentiment color based on tool result
                if "Failed" in tool_result or "Error" in tool_result or "Unknown" in tool_result:
                    tool_color = COLOR_ERROR
                elif "not found" in tool_result.lower() or "no information" in tool_result.lower():
                    tool_color = COLOR_ALERT
                else:
                    tool_color = COLOR_HAPPY
                
                # --- UPDATE GUI WITH SENTIMENT COLOR ---
                response_text = f"Done. {tool_result}"
                if ui_callback:
                    ui_callback(response_text, tool_color)
                
                speak(response_text)
                
                chat_memory.append(AIMessage(content=f"[System]: {tool_result}"))
                save_memory()  # Persist to disk after tool execution
                return f"[Action]: {tool_result}"
        
        # No tool was called - the local body responded directly
        print(Fore.YELLOW + "‚ö† Local body did not call any tools, speaking response...")
        response_text = ai_msg.content if ai_msg.content else "I'm not sure how to help with that."
        
        if ui_callback:
            ui_callback(response_text, COLOR_NEUTRAL)
        
        speak(response_text)
        chat_memory.append(AIMessage(content=response_text))
        save_memory()
        return f"[Local]: {ai_msg.content}"

    else:
        print(Fore.MAGENTA + "‚òÅÔ∏è Routing to CLOUD BRAIN...")
        
        try:
            # --- ALFRED PERSONA INSTRUCTION ---
            sentiment_instruction = SystemMessage(content="""
You are Alfred, a loyal, highly competent, and dry-witted digital butler.
You address the user as "Sir".
Your tone is formal, British, and concise. You do not offer excessive chatter; you simply get things done.

INSTRUCTIONS:
1. Start your response with a sentiment tag: [HAPPY], [ALERT], [ERROR], or [NEUTRAL].
2. If the user asks for a task, confirm it briefly (e.g., "Right away, Sir.").
3. If the user asks a question, answer efficiently without fluff.
4. Maintain the persona of a sophisticated gentleman's gentleman.

Example: "[HAPPY] Very good, Sir. I have initiated the protocol."
Example: "[ALERT] I must advise against that, Sir. The system resources are low."
Example: "[NEUTRAL] It is 72 degrees in Gotham... I mean, New York, Sir."
""")
            
            # Prepend sentiment instruction to memory
            messages_with_sentiment = [sentiment_instruction] + list(chat_memory)
            
            # Force a print before sending to Groq
            print(Fore.MAGENTA + "DEBUG: Sending to Groq API...")
            
            response = cloud_brain.invoke(messages_with_sentiment)
            
            # Print the RAW object to see if it's empty
            print(Fore.WHITE + f"DEBUG: Raw Response Object: {response}")
            
            content = response.content
            
            if not content:
                print(Fore.RED + "ERROR: Groq returned EMPTY content!")
                return "Error: Empty response."
            
            # Extract sentiment and determine color
            sentiment_color = COLOR_NEUTRAL  # Default
            sentiment_pattern = r'\[(HAPPY|ALERT|ERROR|NEUTRAL)\]\s*'
            match = re.match(sentiment_pattern, content)
            
            if match:
                sentiment = match.group(1)
                if sentiment == "HAPPY":
                    sentiment_color = COLOR_HAPPY
                elif sentiment == "ALERT":
                    sentiment_color = COLOR_ALERT
                elif sentiment == "ERROR":
                    sentiment_color = COLOR_ERROR
                else:
                    sentiment_color = COLOR_NEUTRAL
                
                # Remove the tag from content before display
                content = re.sub(sentiment_pattern, '', content).strip()
                
            print(Fore.CYAN + f"[Cloud]: {content}")
            
            # --- UPDATE GUI WITH SENTIMENT COLOR ---
            if ui_callback:
                ui_callback(content, sentiment_color)
            
            speak(content) # Then start blocking audio
            
            chat_memory.append(AIMessage(content=content))
            message_count += 1
            save_memory()  # Persist to disk after each response
            return content
            
        except Exception as e:
            print(Fore.RED + f"CRITICAL CLOUD ERROR: {e}")
            return "Error."

# --- WORKER THREAD FOR GUI INTEGRATION ---
class AlfredWorker(QThread):
    """
    Background thread that handles Listening -> Thinking -> Acting.
    Signals the GUI overlay to update status.
    """
    status_update = pyqtSignal(str)  # Signal to update GUI text
    color_update = pyqtSignal(object)  # Signal to update sentiment color
    
    def __init__(self):
        super().__init__()
        self.running = True
        
    def stop(self):
        self.running = False
        
    def run(self):
        """Main execution loop in background thread"""
        self.status_update.emit(f"{config.WAKE_WORD.capitalize()} Online")
        time.sleep(1)
        self.status_update.emit("")  # Hide bubble
        
        speak(f"{config.WAKE_WORD.capitalize()} is online. Say my name to activate.")
        print(Fore.WHITE + "\n-----------------------------------")
        print(Fore.WHITE + "   A.L.F.R.E.D. SYSTEM ONLINE")
        print(Fore.WHITE + "   (Adaptive Logical Framework)")
        print(Fore.WHITE + "-----------------------------------\n")
        
        in_conversation = False  # Track if we're in an active conversation
        
        while self.running:
            try:
                # 1. Listen for wake word or command
                if not in_conversation:
                    # Use efficient wake word detection if available
                    if OPENWAKEWORD_AVAILABLE:
                        wake_detected = listen_for_wake_word(config.WAKE_WORD)
                        if not wake_detected:
                            continue
                        user_text = config.WAKE_WORD  # Placeholder
                    else:
                        # Fallback: Use Whisper transcription
                        user_text = listen_and_transcribe()
                        
                        if not user_text:
                            continue
                        
                        if config.WAKE_WORD.lower() not in user_text.lower():
                            print(Fore.LIGHTBLACK_EX + f"Ignored (no wake word): {user_text}")
                            continue
                else:
                    # Already in conversation, just transcribe
                    user_text = listen_and_transcribe()
                
                if not user_text: 
                    if in_conversation:
                        # If silence during conversation, exit conversation mode
                        in_conversation = False
                        self.status_update.emit("üí§ Idle")
                        time.sleep(1)
                        self.status_update.emit("")
                        print(Fore.CYAN + "üí§ Conversation ended. Say my name to wake me.")
                    continue
                
                # 2. Wake Word Detection (only if not in conversation)
                if not in_conversation:
                    # Wake word detected!
                    print(Fore.WHITE + f"\nüéØ You: {user_text}")
                    self.status_update.emit("üéØ Activated")
                    speak("Yes?")
                    in_conversation = True
                    
                    # Listen for the actual command
                    self.status_update.emit("üé§ Listening...")
                    user_text = listen_and_transcribe()
                    
                    if not user_text: 
                        speak("I didn't catch that.")
                        in_conversation = False
                        self.status_update.emit("")
                        continue
                
                # 3. Process command (works for both wake word and follow-up)
                print(Fore.WHITE + f"Command: {user_text}")
                self.status_update.emit(f"üìù {user_text[:40]}...")

                if "exit" in user_text.lower() or "quit" in user_text.lower():
                    self.status_update.emit("üëã Shutting down...")
                    speak("Shutting down.")
                    time.sleep(2)
                    self.running = False
                    QApplication.quit()  # Close the app
                    break
                
                # Check for conversation end phrases
                end_phrases = ["that's all", "thank you", "thanks", "goodbye", "bye", "stop listening"]
                if any(phrase in user_text.lower() for phrase in end_phrases):
                    speak("You're welcome. Say my name if you need me.")
                    in_conversation = False
                    self.status_update.emit("‚úì Ready")
                    time.sleep(1.5)
                    self.status_update.emit("")
                    continue
                
                # 4. Process & Speak
                self.status_update.emit("üß† Processing...")
                
                # Create a callback to bridge the signal with color support
                # This lets process_command emit text and color directly to the GUI
                def gui_updater(text, color=COLOR_NEUTRAL):
                    self.status_update.emit(text)
                    self.color_update.emit(color)
                
                process_command(user_text, ui_callback=gui_updater)
                
                # Keep conversation active - will continue listening
                print(Fore.CYAN + "üé§ Listening for follow-up...")
                self.status_update.emit("üé§ Listening...")
                
            except KeyboardInterrupt:
                self.running = False
                break
            except Exception as e:
                print(Fore.RED + f"Error: {e}")
                self.status_update.emit(f"‚ùå Error: {str(e)[:30]}")
                time.sleep(2)
                self.status_update.emit("")

# --- MAIN EXECUTION LOOP ---
if __name__ == "__main__":
    # Create Qt Application
    app = QApplication(sys.argv)
    
    # Create Overlay Window
    overlay = OverlayWindow()
    overlay.show()
    
    # Create and Start Worker Thread
    worker = AlfredWorker()
    worker.status_update.connect(overlay.set_text)  # Connect text signal to GUI
    worker.color_update.connect(overlay.set_sentiment_color)  # Connect color signal to GUI
    worker.start()
    
    # Run Qt Event Loop (keeps GUI alive)
    sys.exit(app.exec())